{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Scaling Laws Analysis\n",
    "\n",
    "This notebook demonstrates how to use the Random Forest scaling laws research framework to analyze how Random Forest performance scales with different computational resources.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll explore:\n",
    "1. How training time scales with dataset size (samples and features)\n",
    "2. How training time scales with Random Forest parameters\n",
    "3. Memory usage patterns\n",
    "4. Scaling law analysis and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scaling_laws_research.utils.config import ExperimentConfig, RandomForestConfig, DataConfig\n",
    "from scaling_laws_research.experiments.scaling_experiments import ScalingExperiment\n",
    "from scaling_laws_research.analysis.scaling_laws import ScalingLawAnalyzer\n",
    "from scaling_laws_research.visualizations.scaling_plots import ScalingPlotter\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure Experiment\n",
    "\n",
    "First, let's set up a simple experiment configuration. We'll use smaller scales for the notebook to keep runtime reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom configuration for the notebook\n",
    "config = ExperimentConfig(\n",
    "    name=\"notebook_scaling_demo\",\n",
    "    description=\"Random Forest scaling demo for notebook\",\n",
    "    output_dir=\"../results/notebook_demo\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Customize data configuration for faster execution\n",
    "config.data.base_samples = 500\n",
    "config.data.base_features = 10\n",
    "config.data.sample_scales = [1, 2, 4, 8]  # Smaller scales\n",
    "config.data.feature_scales = [1, 2, 4]    # Smaller scales\n",
    "\n",
    "# Customize Random Forest parameters for faster execution\n",
    "config.random_forest.n_estimators = [10, 50, 100]  # Fewer trees\n",
    "config.random_forest.max_depth = [None, 5, 10]     # Fewer depth options\n",
    "config.random_forest.min_samples_split = [2, 5]    # Fewer options\n",
    "config.random_forest.min_samples_leaf = [1, 2]     # Fewer options\n",
    "\n",
    "print(\"Experiment configuration:\")\n",
    "print(f\"Base samples: {config.data.base_samples}\")\n",
    "print(f\"Base features: {config.data.base_features}\")\n",
    "print(f\"Sample scales: {config.data.sample_scales}\")\n",
    "print(f\"Feature scales: {config.data.feature_scales}\")\n",
    "print(f\"Total dataset combinations: {len(config.data.sample_scales) * len(config.data.feature_scales)}\")\n",
    "print(f\"Total parameter combinations: {len(config.get_parameter_grid())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Scaling Experiment\n",
    "\n",
    "Now let's run the experiment to collect scaling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the experiment\n",
    "experiment = ScalingExperiment(config)\n",
    "results_df = experiment.run_full_experiment()\n",
    "\n",
    "print(f\"\\nExperiment completed! Collected {len(results_df)} data points.\")\n",
    "print(\"\\nFirst few results:\")\n",
    "print(results_df[['experiment_id', 'scaling_type', 'n_samples_train', 'n_features', 'training_time', 'accuracy']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Data Scaling Results\n",
    "\n",
    "Let's examine how performance scales with dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data scaling results\n",
    "data_results = results_df[results_df['scaling_type'] == 'data'].copy()\n",
    "\n",
    "print(\"Data Scaling Results:\")\n",
    "print(data_results[['n_samples_train', 'n_features', 'training_time', 'training_memory_mb', 'accuracy']].head(10))\n",
    "\n",
    "# Quick statistics\n",
    "print(\"\\nScaling ranges:\")\n",
    "print(f\"Samples: {data_results['n_samples_train'].min()} - {data_results['n_samples_train'].max()}\")\n",
    "print(f\"Features: {data_results['n_features'].min()} - {data_results['n_features'].max()}\")\n",
    "print(f\"Training time: {data_results['training_time'].min():.3f} - {data_results['training_time'].max():.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Scaling Patterns\n",
    "\n",
    "Let's create visualizations to understand the scaling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plotter and generate data scaling plots\n",
    "plotter = ScalingPlotter()\n",
    "\n",
    "# Plot training time scaling\n",
    "fig = plotter.plot_data_scaling(results_df, metric=\"training_time\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The plots above show how training time scales with:\")\n",
    "print(\"- Number of training samples (top left)\")\n",
    "print(\"- Number of features (top right)\")\n",
    "print(\"- Heatmap of both dimensions (bottom left)\")\n",
    "print(\"- Overall dataset complexity (bottom right)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter scaling\n",
    "fig = plotter.plot_parameter_scaling(results_df, metric=\"training_time\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The plots above show how training time scales with Random Forest parameters:\")\n",
    "print(\"- Number of estimators (trees)\")\n",
    "print(\"- Maximum depth\")\n",
    "print(\"- Minimum samples for split\")\n",
    "print(\"- Minimum samples per leaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Scaling Laws\n",
    "\n",
    "Now let's analyze the scaling behavior mathematically and extract scaling laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze scaling laws\n",
    "analyzer = ScalingLawAnalyzer()\n",
    "data_analysis = analyzer.analyze_data_scaling(results_df)\n",
    "param_analysis = analyzer.analyze_parameter_scaling(results_df)\n",
    "\n",
    "print(\"Data Scaling Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for x_var, metrics in data_analysis.items():\n",
    "    print(f\"\\n{x_var.replace('_', ' ').title()}:\")\n",
    "    for metric, analysis in metrics.items():\n",
    "        if analysis.get('valid', False) and analysis['r_squared'] > 0.5:\n",
    "            print(f\"  {metric}: O(x^{analysis['b']:.2f}) - R² = {analysis['r_squared']:.3f}\")\n",
    "            if 'interpretation' in analysis:\n",
    "                print(f\"    → {analysis['interpretation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the full report\n",
    "report = analyzer.generate_summary_report(data_analysis, param_analysis)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scaling Laws Visualization\n",
    "\n",
    "Let's create detailed plots showing the power-law fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scaling laws plot with power-law fits\n",
    "fig = plotter.plot_scaling_laws(results_df)\n",
    "plt.show()\n",
    "\n",
    "print(\"The plots above show:\")\n",
    "print(\"- Log-log plots of performance metrics vs dataset dimensions\")\n",
    "print(\"- Red dashed lines show power-law fits (y = a × x^b)\")\n",
    "print(\"- Bottom right panel shows scaling exponents and fit quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison\n",
    "\n",
    "Finally, let's compare different performance aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance comparison plots\n",
    "fig = plotter.plot_performance_comparison(results_df)\n",
    "plt.show()\n",
    "\n",
    "print(\"The comparison plots show:\")\n",
    "print(\"- Performance vs training time trade-offs\")\n",
    "print(\"- Memory vs time relationships\")\n",
    "print(\"- Training vs prediction time scaling\")\n",
    "print(\"- CPU usage patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights\n",
    "\n",
    "Let's summarize the key findings from our scaling analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key insights\n",
    "insights = analyzer._extract_key_insights(data_analysis, param_analysis)\n",
    "\n",
    "print(\"Key Insights from Random Forest Scaling Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"This analysis provides empirical scaling laws for Random Forest models,\")\n",
    "print(\"helping to predict computational requirements and optimize resource allocation\")\n",
    "print(\"for machine learning workflows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To extend this analysis:\n",
    "\n",
    "1. **Larger Scale Experiments**: Run with larger datasets and more parameter combinations\n",
    "2. **Different Datasets**: Test with various real-world datasets to see how scaling laws generalize\n",
    "3. **Other Algorithms**: Compare Random Forest scaling with other ensemble methods\n",
    "4. **Hardware Analysis**: Study how scaling laws change with different hardware configurations\n",
    "5. **Parallel Processing**: Analyze scaling with different numbers of CPU cores\n",
    "\n",
    "The framework is designed to be extensible - you can easily modify configurations and add new metrics for analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}